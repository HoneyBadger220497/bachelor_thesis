Um eine möglichst einfache beziehungsweise brauchbare Darstellung für die Zustandssumme des Ising-Modells zu finden, werden die sogenannten Graßmann Zahlen eingeführt. Um die Berechnung in den folgenden Kapiteln durchführen zu können, sollen in diesem Abschnitt die wichtigsten abstrakten algebraischen Eigenschaften und Operationen für diese Zahlen vorgestellt werden. \\

\subsection{Graßmann Variablen und Graßmann Funktionen}

Als Graßmann Algebra wird eine assoziative $\mathbb C$-Algebra $(\mathcal A, \cdot, +, \wedge)$ mit Eins bezeichnet, welche von einer Familie von Generatoren $(\eta_1, \eta_2, \dots, \eta_n)$  erzeugt wird, die Eigenschaft \eqref{eq: antikommutingProb} erfüllen. 

\begin{equation} \label{eq: antikommutingProb}
\forall\;i,j\in\{1,\dots,n\}: \eta_i \wedge \eta_j = - \eta_j \wedge \eta_i
\end{equation}

\noindent Die Generatoren $(\eta_i)_{i=1}^n$ sind linear unabhängig und werden als Graßmann Variablen bezeichnet. Fortan soll in der Notation, wie es auch in der Literatur üblich ist, die explizite Unterscheidung von $\wedge$ und $\cdot$ in der Notation weggelassen werden. Für eine Familie von $n$ Graßmann Variablen bildet $\mathcal A$ einen $2^n$ dimensionalen Vektorraum. Eine mögliche Basis $\mathcal B$ ist dabei durch die Monome \eqref{def: Monom} in den Graßmann Variablen bis zum Grad $n$ gegeben. Dabei bezeichnet $ p $ den Grad des Monomes.
\begin{equation} \label{def: Monom}
    \mathcal B = \{\eta_{i_1}\,  \eta_{i_2}  \cdots \eta_{i_p} \;| \; i_1 < i_2 < \dots i_p \;\;\text{und}\;\; p\leq n \}  
\end{equation}

\noindent Die Variablen sind für die Monome immer in aufsteigender Reihenfolge angegeben. Im folgenden sei ein Beispiele für $n = 3$ angegeben. Für p = 0 wird das Eins-Element der Algebra als zugehöriges Monom definiert. 
\begin{align}
    p = 0 &: 1 \nonumber \\
    p = 1 &: \eta_1,\; \eta_2,\; \eta_3 \nonumber \\
    p = 2 &: \eta_1\,\eta_2,\; \eta_1\,\eta_3,\; \eta_2,\eta_3 \nonumber \\
    p = 3 &: \eta_1\,\eta_2\,\eta_3 \nonumber 
\end{align}


\noindent  Ein Element $f\in\mathcal A$ kann dann eindeutig auf der Basis als Polynom $n$-ten Grades in den Graßmann Variablen dargestellt werden. $f$ heißt eine Graßmann Zahl oder, aufgrund der Polynomdarstellung, auch Graßmann Funktion. Die historisch begründete Bezeichnung als Variable oder Funktion ist dabei ein wenig irreführend da sowohl die Generatoren als auch die Funktionen fixe Elemente der selben Algebra $\mathcal A$ sind und keine variablen Eigenschaften aufweisen. Jede Graßmann Funktion lässt sich als eindeutige Linearkombination der Monome darstellen, wodurch die allgemeinste Graßmann Funktion durch \eqref{def: GraßmannFunktion} gegeben ist.
\begin{equation} \label{def: GraßmannFunktion}
    f(\eta_1, \dots ,\eta_n) = f_0 + \sum_{i} f_i \;\eta_i + \sum_{i_1<i_2} f_{i_1,i_2} \;\eta_{i_1} \,\eta_{i_2} + \sum_{i_1<i_2<i_3} \dots \;+ f_{1,2,\dots,n} \; \eta_{1}\,\eta_{2}\,\cdots\,\eta_{n}
\end{equation}

\noindent Aus \eqref{def: GraßmannFunktion} ist zudem auch ersichtlich, dass sich jede Graßmann Funktion für jede Variable gemäß \eqref{eq: GraßmannZerlegung} in eindeutiger Weise zerlegen lässt, sodass die Teile $\bar f$ und $f'$ nicht mehr von dieser Variable abhängen. 
\begin{align} 
    f(\eta_1,\dots,\eta_n) &= \bar f(\eta_1,\dots,\eta_{k-1},\eta_{k+1}, \dots ,\eta_n) + \eta_k f'(\eta_1,\dots,\eta_{k-1},\eta_{k+1}, \dots ,\eta_n) \nonumber\\
        & = \bar f + \eta_k f' \label{eq: GraßmannZerlegung}
\end{align}

\noindent In \eqref{eq: GraßmanProp 2} und \eqref{eq: GraßmanProp 3} sind noch einige leicht einsehbare Rechenregeln für die Monome gegeben. Für p=1 ergibt sich der Spezialfall einzelner Grassmann Variablen.
\begin{grayframe}
    Seien $\phi_p$ und $\psi_q$ zwei Monome vom Grad $p$ bzw. $q$ so gilt:
    \begin{align}
        \phi_p^2 &= 0  \label{eq: GraßmanProp 2}\\
        \phi_p \, \psi_q &= \left\{ \begin{array}{ll} 0 & \text{für}\; p+q > n \\ (-1)^{pq} \psi_q \phi_p  & \text{für}\; p+q \leq n\end{array} \right. \label{eq: GraßmanProp 3}
    \end{align}
\end{grayframe}

\noindent Die Reihendarstellungen von komplexen analytische Funktionen können benutzt werden um Graßmann Funktionen zu definieren. Aufgrund der Eigenschaften \eqref{eq: GraßmanProp 2} brechen diese immer nach einer endlichen Anzahl von Termen ab. Dies ist im folgenden für die Exponentialfunktion vorgeführt. 
\begin{equation} \label{eq: exp()}
    f(\eta_1) = exp(\eta_1) = 1 + \eta_1 + \frac{\eta_1^2}{2} + \dots = 1 + \eta_1
\end{equation}

\noindent Dies funktioniert für beliebige andere Graßmann Zahlen und ist nicht auf die Generatoren der Algebra beschränkt. Es kann zum Beispiel ein beliebiges Monom eingesetzt werden, welches ja selbst eine Graßmannfunktion der Generatoren ist. 
\begin{equation} \label{eq: exp()}
    f(\phi_p) = exp(\phi_p) = 1 + \\phi_p + \frac{\phi_p^2}{2} + \dots = 1 + \phi_p
\end{equation}

\noindent Es ist jedoch darauf zu achten dass sich diese Exponentialfunktion nicht immer verhält, wie man es von den Reellen Zahlen gewohnt ist. Seien dazu $\phi_p$ und $\psi_q$ Monome in den Graßmann Variablen vom Grad $p$ und $q$; 
\begin{align}
    f(\phi_p, \psi_q) 
        &= exp(\phi_p + \psi_q) \nonumber \\
        &= 1 + (\phi_p + \psi_q) + \frac{1}{2}(\phi_p + \psi_q)^2 + \dots \nonumber\\
        &= 1 + (\phi_p + \psi_q) + \frac{1}{2}(\phi_p^2 + \phi_p\psi_q +\psi_q\phi_p + \psi_q^2) + \dots \nonumber\\
     &= 1 + (\phi_p + \psi_q) + \frac{1}{2}( \phi_p\psi_q +\psi_q\phi_p) \\
    \nonumber \\
    \bar{f}(\phi_p, \psi_q) 
        &= exp(\phi_p) exp(\psi_q) \nonumber \\
        &= (1 + \phi_p)(1 +\psi_q) \nonumber \\
    &= 1+  \phi_p + \psi_q + \phi_p\psi_q 
\end{align}

\noindent Man erkennt mithilfe von \eqref{eq: GraßmanProp 3} dass $f = \bar f$ für $p=q = 2$ gilt, nicht jedoch für $ p = q = 1 $.

\subsection{Spur Operation und Berezin Integral}

Als nächstes soll eine lineare Operation $\textit{Sp}:\mathcal A \rightarrow \mathbb C$ eingeführt werden, welche als Spur einer Graßmann Zahl bezeichnet wird. Dazu wird das sogenannte Berezin Integral für Graßmann Variablen verwendet. Dieses Integral ist als eine lineare Operation $\int: \mathcal A \rightarrow \mathcal A$ zu verstehen und hat nichts, außer einigen algebraischen Eigenschaften, mit dem lebesgue'schen oder riemannschen Integralbegriff zu tun. Die Vorschrift \eqref{def: Berezin} definiert über die Zerlegung \eqref{eq: GraßmannZerlegung} das Berenzin Intergal in einer Graßmann Variable auf ganz $\mathcal A$ in eindeutiger Weise.
\begin{align} 
\int d \eta_i \; f = \int d \eta_i \; \bar{f} + \eta_i f' &= f' \label{def: Berezin}
\end{align}

\noindent Als Beispiel ist in \eqref{bsp: BerezinRechnung} diese Integration in der 2-ten Variable für eine Graßmann Funktion $f$ auf einer Algebra mit 2 Generatoren $\eta_1, \eta_2$ vorgeführt.
\begin{align} 
\int d\eta_2 f(\eta_1,\eta_2,\eta_n) &= \int d\eta_2 \; f_0 + f_1 \eta_1 + f_2 \eta_2 + f_{1,2} \eta_1 \eta_2 \nonumber \\
&= \int d\eta_2 \; (f_0 + f_1 \eta_1) + \eta_2 (f_2  - f_{1,2} \eta_1) \nonumber\\
&= f_2  - f_{1,2} \eta_1 \label{bsp: BerezinRechnung}
\end{align}

\noindent Dabei ist zu beachten, dass das negative Vorzeichen von $f_{1,2}$ von den antikommutierenden Eigenschaften der Variablen bei der Zerlegung in $\bar f$ und  $f'$ herrührt. In \eqref{def: Berezin} und \eqref{bsp: BerezinRechnung} fällt auf, dass die Berezin Integraloperation sich wie eine algebraische Ableitungsvorschrift für Polynome verhält. Einige leicht einzusehenden Eigenschaften des Berezin-Integrals sind in \eqref{eq: Berezin1} bis \eqref{eq: Berezin3} zusammengefasst. Oftmals werden diese als definierende Eigenschaften heran gezogen und daraus die Vorschrift \eqref{def: Berezin} abgeleitet. 
\begin{align}
    \int d\eta_i 1 &= 0 \label{eq: Berezin1}\\
    \int d\eta_i \eta_j &= \delta_{i,j} \\
    \int d\eta_i \eta_j \eta_k &=  \delta_{i,j} \eta_k = - \delta_{i,k} \eta_j \label{eq: Berezin3}
\end{align}

\noindent Ein Berezin Integral über mehrere Variablen ist dann über die Hintereinanderausführung der Integrale bezüglich einzelnen Variablen definiert. 
\begin{equation}
\iint d \eta_{i}\, d \eta_{j}\; f := \int d \eta_{i} \left( \int d \eta_{j}\; f\; \right)
\end{equation}

\noindent Aus den antikommutativen Eigenschaften der Graßmann Variablen und der Definition des Integrals folgt damit auch eine Antikommutations-Relation für die Mehrfachintegrale. Das Vorzeichen hängt dabei dann von der Integrationsreihenfolge ab. Dies wird für Mehrfachintegral durch die Reihenfolge der Variablen im ``Integrationsmaß'' festgehalten.
\begin{equation}
\iint d \eta_{i}\, d \eta_{j}\; f = - \iint d \eta_{j}\, d \eta_{i}\; f
\end{equation}

\noindent Integriert man dann in aufsteigener Reihenfolge über alle Variablen so ergibt sich der Koeffizient des höchsten Monoms in der Basisdarstellung von $f$. Dieser Koeffizient liegt zudem in $\mathbb C$. Durch ein derartiges Berezin integral wird die Spur-Operation gemäß \eqref{def: GraßmannSpur} definiert. 
\begin{grayframe}[frametitle = {Definition: Spur-Operation}]
    \begin{equation} \label{def: GraßmannSpur}
        \forall \;f\in \mathcal A : \;\;\;\; \Sp f := \int \cdots \int d\eta_n\,d\eta_{n-1}\cdots d\eta_1\, f = f_{1,2,\dots,n}
    \end{equation}
\end{grayframe}

\noindent Nun soll das Verhalten der Spur-Operation unter einer linearen Variablensubstitution, wie in \eqref{def: Graßmann_linearTransform_Matrix} bzw. \eqref{def: Graßmann_linearTransform}, untersucht werden. 
\begin{equation} \label{def: Graßmann_linearTransform_Matrix}
    \bm{\eta}\left(\bm{\theta}\right) = \bm{M}\,\bm{\theta}
\end{equation}
\begin{equation} \label{def: Graßmann_linearTransform}
    \eta_i\left( \theta_1,\dots,\theta_n \right) = \sum_{j=1}^n M_{i,j}\; \theta_j
\end{equation}

\noindent Ist $M$ regulär so bildet eines solche Transformation eine Familie $\left(\theta_i\right)_{i = 1}^n$ von Graßmann-Variablen auf eine andere Familie $\left(\eta_i\right)_{i = 1}^n$ von Generatoren der Algebra ab. Dies ist dadurch zu erkennen, dass invertierbare Abbildungen die lineare Unabhängigkeit erhalten und die Eigenschaft der Antikommutativität nach \eqref{eq: conservation of antikommutativity} erhalten bleibt.
\begin{alignat}{4}
    \eta_i \,\eta_j 
        &= \left(\sum_{k=1}^n M_{i,k} \; \theta_k  \right) \left(\sum_{l=1}^n M_{j,l} \;\theta_l\right)     
        &&= \sum_{k=1}^n \sum_{l=1}^n\; M_{i,k}\; M_{j,l} \;\theta_k \;\theta_l &&& \nonumber \\
        &= \sum_{k=1}^n \sum_{l=1}^n M_{i,k} M_{j,l} \left(-\theta_l \theta_k \right)\
        &&= - \sum_{k=1}^n \sum_{l=1}^n M_{j,l}\; M_{i,k} \;\theta_l \;\theta_k &&&\;= - \eta_j  \,\eta_i \label{eq: conservation of antikommutativity}
\end{alignat}

\noindent Die beiden Familien von Graßmann-Variablen sind äquivalent in der Hinsicht, dass alle bislang hergeleiteten Eigenschaften für jede Familie von Generatoren gilt. Für die Monome gilt, dass jedes Monom vom Grad $p$ auf ein Polynom vom gleichen Grad in den neuen Graßmann Variablen abgebildet wird.
$$ \eta_{i_1}, \dots, \eta_{i_p} = \sum_{{j_1}=1}^n\;\cdots \sum_{{j_p}=1}^n M_{{i_1},{j_1}}\cdots M_{{i_p},{j_p}}\;\theta_{j_1} \cdots \theta_{j_p} $$

\noindent Durch den Versuch ein beliebiges Produkt von Graßmann Variablen durch paarweise Vertauschung in ihre natürliche Reihenfolge umzuordnen, macht man sich leicht den Zusammenhang \eqref{eq: sort_graßmann} mit dem Levi-Civita-Symbol (Siehe Anhang) klar.   
\begin{equation} \label{eq: sort_graßmann}
\theta_{j_1} \cdots \theta_{j_p} = \epsilon_{j_1,\dots,j_n} \, \theta_{1} \cdots \theta_{n}
\end{equation}

\noindent Für eine beliebige Graßmann Funktion ergibt sich dann \eqref{eq: transform last coeff} unter der Transformation. Dabei ist $\bar f$ ein Polynom von höchstens Grad $n-1$.
\begin{align}
f(\bm{\eta}\,(\theta_1, \dots, \theta_n))
&= \bar{f} + f_{1,\dots,n}\;\eta_1(\theta_1, \dots, \theta_n)\cdots\eta_n(\theta_1, \dots, \theta_n) \nonumber \\
&= \bar{f} + f_{1,\dots,n} \sum_{{j_1}=1}^n\;\cdots \sum_{{j_n}=1}^n M_{{1},{j_1}}\cdots M_{{n},{j_n}}\;\theta_{j_1} \cdots \theta_{j_n} \nonumber \\
&= \bar{f} + f_{1,\dots,n} \left(\sum_{{j_1}=1}^n\;\cdots \sum_{{j_n}=1}^n M_{{1},{j_1}}\cdots M_{{n},{j_n}}\; \epsilon_{j_1,\dots,j_n}\right) \theta_{1} \cdots \theta_{n} \nonumber \\
&= \bar{f} + f_{1,\dots,n} \; det(M) \; \theta_{1} \cdots \theta_{n} \label{eq: transform last coeff} 
\end{align}

\noindent Mit der Notation $D_{\eta} = d\eta_n\,d\eta_{n-1}\,\cdots d\eta_1 $ und unter der Beachtung der Definition \eqref{def: GraßmannSpur} der Spuroperation erhält man die Relation \eqref{eq: Graßmann Transformationsformel}, welche zur Transformationsformel für Riemannintegrale ähnlich ist. Im Gegensatz zum Riemannintegral ist jedoch die Determinante der Transformationsmatrix $M$ zu bilden, nicht die der Inversen $M^{-1}$. 
\begin{grayframe}[frametitle = {Für Variablentransformationen $\bm{\eta}\left(\bm{\theta}\right) = M\,\bm{\theta}$ gilt:}]
\begin{align} \label{eq: Graßmann Transformationsformel}
    \int D_{\bm{\theta}}\, f\left(\bm{\eta}\left(\bm{\theta}\right)\right)  &= det(M) \int D_{\bm{\eta}}\, f(\bm{\eta}\,) \\
    \nonumber \\
    \Sp{f( M\,\bm{\theta}\,)} &=  det(M) \,\Sp{f(\bm{\eta}\,)}
\end{align}
\end{grayframe}


\subsection{Gauß-Berezin-Integrale und Pfaffsche Determinante}

In diesem Abschnitt sollen Integrale der Form \eqref{def: Gaussintegral} betrachtet werden, da diese immer wieder in der Arbeit auftauchen werden. Integrale diesen Typs sollen fortan als Gauß-Berezin-Integrale bezeichnet werden. Die Größe $A\left(\bm{\eta}, \bm{\eta}\right)$ aus \eqref{def: QuadratWirkung} wird als quadratische Wirkung in den Graßmann Variablen oder als quadratische Graßmann Wirkung bezeichnet. $\bm{A}$ ist dabei die Darstellende Matrix der Quadratischen Form bzgl. der Familie von Generatoren $\eta_1, ... \eta_n$. Die Graßmann Funktion $e^{A/2}$ wird, in Anlehnung an die Wahrscheinlichkeitstheorie, als Gauß-Graßmann-Dichte bezeichnet.   
\begin{equation} \label{def: Gaussintegral}
\int \cdots \int d\eta_n\,d\eta_{n-1}\cdots d\eta_1\, \exp{\frac{A\left(\bm{\eta}, \bm{\eta}\right)}{2}}
\end{equation}
\begin{equation} \label{def: QuadratWirkung}
A\left(\bm{\eta}, \bm{\eta}\right) = \bm{\eta}^{\,T} \bm{A}\, \bm{\eta} = \sum_{i=1}^n \sum_{j=1}^n A_{i,j}\; \eta_i\; \eta_j 
\end{equation}

\noindent Die Matrix $\bm{A}^T$ steht hierbei für die Transponierte von $\bm{A}$. Diese ist nicht zu verwechseln mit der adjungierten $\bm{A}^\dagger$. Im folgenden wird immer nur die Transposition verwendet, egal ob $\bm{A}$ komplex oder reell ist. Eine mit diesen Integralen eng zusammenhängende Größe ist die sogenannte Pfaffsche Determinante $pf(\bm{A})$ einer antisymmetrischen Matrix. Für eine antisymmetrische Matrix $\bm{A}$ der Größe $2n \times 2n$ ist diese wie in \eqref{def: Pfaffian} definiert. Für antisymetrische quadratische Matrizen $\bm{A}$ ungerader Dimension gilt $pf(\bm{A}) = 0$. 
\begin{equation} \label{def: Pfaffian}
pf(\bm{A}) = \frac{1}{2^n\,n!} \sum_{\sigma \in S_{2n}} sign(\sigma) \prod_{i=1}^n A_{\sigma(2i-1),\sigma(2i)} 
\end{equation}

\noindent Die Summation in \eqref{def: Pfaffian} wird dabei über alle Permutationen $\sigma$ der Zahlen $1$ bis $2n$ ausgeführt. $S_{2n}$ bezeichnet die zugehörige Symetrische Gruppe, also die Menge aller solcher Permutationen. Mit $sign(\sigma)$ wird das Vorzeichen der Permutation $\sigma$ bezeichnet. \\

\noindent Für eine $2\times2$ bzw. eine $4\times4$ Matrix kann die Pfaffsche Determinate leicht im allgemeinen Fall, wie in \eqref{eq: expl. pfaffian}, angegeben werden.
\begin{align}
pf\left(\begin{array}{cc}  
        0  &  a  \\
        -a &  0  \\
        \end{array} \right) &= a \nonumber \\
\label{eq: expl. pfaffian}\\
pf\left(\begin{array}{cccc}  
        0  &  a  &  b & c \\
        -a &  0  &  d & e \\
        -b & -d  &  0 & f \\
        -c & -e  & -f & 0 \\
        \end{array} \right) &= af - be +dc \nonumber 
\end{align}

\noindent Es besteht der folgende Zusammenhang zwischen einem Gauß-Berezin-Integral und den Pfaffschen Determinanten:

\begin{grayframe}[frametitle = {Gauß-Berezin-Integrale}]
Für jede quadratische Graßmann Wirkungen $A$, gibt es genau ein $ \mathbf{A}\in\mathbb{C}^{nxn}$ mit $\mathbf{A}^T = -\mathbf{A}$ sodass gilt :
    \begin{equation} \label{Satz: Gauß-Berezin 1}
        A\left(\bm{\eta}, \bm{\eta} \right) = \sum_{i=1}^n \sum_{j=1}^n a_{i,j}\; \eta_i\; \eta_j  \;\;
    \end{equation}
Dann gilt für das zugehörige Gauß-Berezin-Integral der quadratischen Graßmann Wirkung:
    \begin{equation} \label{Satz: Gauß-Berezin 2}
        \Sp{e^{A/2}} = \int \cdots \int d\eta_n\,d\eta_{n-1}\cdots d\eta_1\, \exp{\frac{A\left(\bm{\eta}, \bm{\eta}\right)}{2}} = pf(\mathbf{A}) 
    \end{equation}
\end{grayframe}

\noindent Zuerst soll gezeigt werden, dass jede quadratische Graßmann Wirkung genau eine schief symmetrische Darstellende Matrix besitzt. Dies kann man einsehen, indem man eine beliebige symmetrische Matrix $\bm{B}$ betrachtet. Die durch $\bm{B}$ definierte quadratische Wirkung $B$ erweist sich dann, gemäß \eqref{eq: Symetrische Wirkung}, als das Null-Element der Graßmann Algebra. 
\begin{align}
B\left(\bm{\eta}, \bm{\eta}\right) 
    & = \sum_{i=1}^n \sum_{j=1}^n B_{i,j}\; \eta_i\; \eta_j = \sum_{i=1}^n \sum_{j=1}^n \frac{1}{2} (B_{i,j} + B_{j,i})\; \eta_i\; \eta_j \nonumber \\
    & =  \frac{1}{2}\left( \sum_{i=1}^n \sum_{j=1}^n B_{i,j}\; \eta_i\; \eta_j - \sum_{i=1}^n \sum_{j=1}^n B_{j,i}\; \eta_j\; \eta_i\right)  = 0 \label{eq: Symetrische Wirkung}
\end{align}

\noindent Sodass sich, durch aufteilen einer Matrix $\bm{A}$ in einen Symmetrischen $\frac{\bm{A}+\bm{A}^T}{2}$ und antisymmetrischen Anteil $\frac{\bm{A}-\bm{A}^T}{2}$, immer eine antisymmetrische Darstellende Matrix ergibt. Diese ist dann eindeutig. Denn gäbe es zwei antisymmetrische Darstellende Matrizen $\bm{A}$ und $\bm{B}$, so folgt: 
\begin{equation} 
A\left(\bm{\eta}, \bm{\eta}\right)  - B\left(\bm{\eta}, \bm{\eta}\right)  = \sum_{i=1}^n \sum_{j=1}^n (A_{i,j} - B_{i,j})\; \eta_i\, \eta_j = 0 \nonumber
\end{equation}

\noindent Wegen der linearen unabhängigeit der Monome $\eta_i\,\eta_j$ gilt dann aber dass alle Koeffizienten verschwinden müssen und somit $\bm{A}=\bm{B}$. \\
\noindent Sei nun $\bm{A} \in\mathbb{C}^{2n \times 2n}$ die eindeutige, antisymmetrische Darstellende Matrix einer quadratischen Graßmann Wirkung $A$aus einer, von $2n$ erzeugten Graßmann Variablen, Algebra. Um die Integration auszuführen, muss eine geeignete Polynom-Darstellung von $ \Sp{e^{A/2}}$ berechnet werden. Dazu wird die Definition der Exponentialfunktion als Potenzreihe benutzt. 
\begin{align}
e^{A/2} 
&= \exp{ \sum_{i=1}^{2n} \sum_{j=1}^{2n} \frac{A_{i,j}}{2} \eta_i \eta_j } 
= \sum_{n=1}^{\infty} \frac{1}{2^n\,n!} \left(\sum_{i=1}^{2n} \sum_{j=1}^{2n} A_{i,j} \eta_i \eta_j \right)^n  \nonumber \\
& = \frac{1}{2^n\,n!} \left(\sum_{i=1}^{2n} \sum_{j=1}^{2n} A_{i,j} \eta_i \eta_j \right)^n  + \sum ' \nonumber\\
&= \frac{1}{2^n\,n!} \sum_{\sigma \in S_{2n}} \, \prod_{i=1}^n  A_{\sigma(2i-1),\sigma(2i)}\eta_{\sigma(2i-1)}\eta_{\sigma(2i)} + \sum ' \label{eq: exp_polynom}
\end{align}

\noindent Dabei steht $\sum'$ für eine Summe von Monomen vom Grad höchstens $n-1$. Monome vom Grad echt größer als $n$ verschwinden aufgrund der Eigenschaft \eqref{eq: GraßmanProp 3}. Die Summe $\sum'$ verschwindet unter der Spurbildung. Unter Berücksichtigung der Linearität der Spuroperation folgt somit:
\begin{align}
\Sp{e^{A/2}} &= \Sp{A_{\sigma(2i-1),\sigma(2i)}\eta_{\sigma(2i-1)}\eta_{\sigma(2i)} + \sum '} \nonumber \\
&= \frac{1}{2^n\,n!} \sum_{\sigma \in S_{2n}} \, \Sp{ \prod_{i=1}^n  A_{\sigma(2i-1),\sigma(2i)}\eta_{\sigma(2i-1)}\eta_{\sigma(2i)}} + \Sp{\sum'} \nonumber \\
&= \frac{1}{2^n\,n!} \sum_{\sigma \in S_{2n}} \, \Sp{ \prod_{i=1}^n \eta_{\sigma(2i-1)}\eta_{\sigma(2i)}} \prod_{i=1}^n A_{\sigma(2i-1),\sigma(2i)}  \label{eq: PfaffCalc_2} 
\end{align}

\noindent Mithilfe von Glg. \eqref{eq: sort_graßmann} lässt sich leicht für eine beliebige Permutation $\sigma \in S_{2n}$ der Zusammenhang \eqref{eq: permuteVariableproducts} einsehen.
\begin{align} \label{eq: permuteVariableproducts}
sign(\sigma) = \Sp{\eta_{\sigma(1)} \cdots \eta_{\sigma(2n)}}  = \,\Sp{ \prod_{i=1}^n \eta_{\sigma(2i-1)}\eta_{\sigma(2i)} }
\end{align}

\noindent Mithilfe von \eqref{eq: permuteVariableproducts} erkennt man in \eqref{eq: exp_polynom} die Definition der Pfaffschen Determinante. Somit ist die Aussage für $\bm{A} \in\mathbb{C}^{2n \times 2n}$ gezeigt. Für $\bm{A} \in\mathbb{C}^{2n+1 \times 2n+1}$ gilt $e^{A/2} = 0$ (Beweis im Anhang).\\

\noindent In \eqref{eq:pfaff 1} bis \eqref{eq:pfaff 6} sind noch einige wichtige Eigenschaften von pfaffschen Determinanten festgehalten. Diese können Eigenständig über die Definition \eqref{def: Pfaffian} der pfaffschen Determinante bewiesen werden. Beweise können zum Beispiel in \cite{} gefundnen werden. Die Definition über das Berezin Integral bildet jedoch einen alternativen, sehr eleganten Zugang, um all diese Beweise zu führen. Hier sollen nur die Resultate zusammengefasst werden, die Beweise befinden sich im Anhang.

\begin{grayframe}[frametitle = {Eigenschaften von Pfaffschen Determinanten}]
    Sei $\bm{A} \in \mathbb{C}^{2n\times2n}$ eine schiefsymmetrische Matrix, dh $\bm{A}^T = -\bm{A}$. so gilt
        \begin{align}
            pf(-\bm{A}) &= (-1)^n pf(\bm{A}) \label{eq:pfaff 1}\\
            pf(\bm{A})^2 &= det(\bm{A})      \label{eq:pfaff 2}
        \end{align}
    Sei $\bm{M} \in \mathbb{C}^{2n\times2n} $ eine beliebige invertierbare Matrix. so gilt:
        \begin{equation}
            pf(\bm{M}^T \,\bm{A}\, \bm{M}) = det(\bm{M}) pf(\bm{A}) \label{eq:pfaff 3}
        \end{equation}
    Seien $\bm{A}_i \in \mathbb{C}^{2n_i\times2n_i}$ schiefsymmetische Matrizen sodass sich $\bm{A}$ als Blockmatrix $\bm{A} = \oplus_{i = 1}^k \bm{A}_i$ schreiben lässt, dann gilt:
        \begin{equation}
            pf(\oplus_{i = 1}^k \bm{A}_i) = \prod_{i=1}^k pf(\bm{A}_i) \label{eq:pfaff 4}
        \end{equation}
    Ist $\bm{A} \in \mathbb{C}^{2n\times2n}$ invertierbar so gilt:
        \begin{equation}
        pf(\bm{A}^{-1}) = (-1)^n pf(\bm{A})^{-1} = pf(-\bm{A})^{-1} \label{eq:pfaff 5}
        \end{equation}
    Sei $\bm{M} \in \mathbb{C}^{n\times n} $ beliebig. So gilt
        \begin{equation}
            pf\left(
                \begin{array}{cc}
                    0    & \bm{M} \\
                    -\bm{M}^T & 0 
                \end{array}
            \right) = (-1)^{n(n-1)/2} \; det(\bm{M}) \label{eq:pfaff 6}
        \end{equation}
\end{grayframe}

\subsection{Graßmann Korrelationen}

Zuletzt sollen Größen der Form \eqref{def: Graßmann-Korrelationen} betrachtet werden. Diese sollen, in  Anlehnung an die Wahrscheinlichkeitstheorie oder Statistische Physik, als Graßmann Korrelationen bezeichnet werden. Diese Größe tritt auch im Pfadintegralformalismus der Quatenfeldtheorie auf. 
\begin{equation} \label{def: Graßmann-Korrelationen}
\corr{f} = \frac{\Sp{e^{A/2} f(\eta_{i_1} \cdots \eta_{i_k})}}{\Sp{e^{A/2}}} 
\end{equation}

\noindent Während der Nenner in \eqref{def: Graßmann-Korrelationen} über eine Pfaffsche Determinante berechnet werden kann, muss für den Zähler noch ein wenig Arbeit geleistet werden. Es soll nun ein Expliziter Ausdruck für den Fall, dass $f$ ein Monom in den Graßmann Variablen ist, abgeleitet werden. Man beachte dass $i_1 < i_2 < \dots < i_{n-k}$ angenommen werden soll.
\begin{equation} \label{def: Graßmann-Korrelationen monom}
\corr{\eta_{i_1} \cdots \eta_{i_k}} = \frac{\Sp{e^{A/2}\eta_{i_1} \cdots \eta_{i_k}}}{\Sp{e^{A/2}}} 
\end{equation}

\noindent Die Korrelation verschwindet sobald eine Graßmann Variable zwei mal vorkommt, da das Produkt dann die 0 ergibt. Es sei $P: \{1,\dots, n\} \mapsto \{i_i, \dots, i_k, j_1, \dots, j_{n-k}\}$ eine Permutation sodass $i_1 < i_2 < \dots < i_{k}$ und $j_1 < j_2 < \dots < j_{n-k}$ gilt. Ist $\bm{P}$ die Darstellende Matrix der Permutation, so ist $det(\bm{P}) = sign(\sigma)$ das Vorzeichen der Permutation. Im folgenden bezeichnet $\bm{A}^{\cancel{I}\cancel{I}}$ die Matrix, die man erhält, wenn man in $\bm{A}$ die Zeilen und Spalten mit den Indices aus $I$ streicht. $\bm{A}^{I \cancel{I}}$ bezeichnet die Matrix die man erhält wenn man in $\bm{A}$ alle Zeilen streicht, deren Index nicht in $I$ liegt und alle Spalten, deren Index doch in $I$ liegen. $\bm{A}^{\cancel{I} I}$ und $\bm{A}^{II}$ sind analog definiert. Um $\bm{A}^{II}$ zu erhalten, streicht man z.b. alle Spalten und Zeilen von $\bm{A}$ deren Index nicht in $I$ liegt.
Durch die Transformation der Matrix $\bm{A}$ mithilfe von $\bm{P}$ erhält man dann:
\begin{equation}
   \bm{\tilde{A}} =  \bm{P}^T \bm{A} \bm{P} =
        \left( \begin{array}{cc}
        \bm{A}^{II}          & \bm{A}^{I \cancel{I}} \\
        \bm{A}^{\cancel{I} I} & \bm{A}^{\cancel{I} \cancel{I}}
        \end{array} \right)
\end{equation}

\noindent Unter dieser Ähnlichkeitstransformation bleibt die Antisymmetrie der Matrix erhalten. Mit der Transformationsformel für das Berezin intergal und der Definionen $ \bm{\theta} = \bm{P} \bm{\eta} $ bzw. $ (\theta_1, \dots, \theta_n) =  (\eta_{i_1}, \dots ,\eta_{i_k}, \eta_{j_1}, \dots, \eta_{i_{n-k}}) $ folgt für die Korrelation: 
\begin{align}
\corr{\eta_{i_1} \cdots \eta_{i_k}} 
    & = \frac{\int D_{\bm{\eta}}\, \exp{ \frac{1}{2}\bm{\eta}^T \bm{A}\, \bm{\eta}\;}  \eta_{i_1}\cdots\eta_{i_k}}{\int D_{\bm{\eta}} \,\exp{\frac{1}{2} \bm{\eta}^T \bm{A}\, \bm{\eta}\;}} \nonumber \\
    & =  \frac{ det(P) \int D_{\bm{\theta}}\, \exp{ \frac{1}{2} \bm{\theta}^T \bm{\tilde{A}}\, \bm{\theta}\;}  \theta_{1}\cdots\theta_k}{det(P) \int D_{\bm{\theta}} \,\exp{ \frac{1}{2} \bm{\theta}^T \bm{\tilde{A}}\, \bm{\theta}\;}} \nonumber \\
    & = \frac{ \int D_{\bm{\theta}}\, \exp{ \frac{1}{2} \bm{\theta}^T \bm{\tilde{A}}\, \bm{\theta}\;}  \theta_{1}\cdots\theta_k}{pf(\bm{\tilde{A}})} \label{eq: transformed Korrelation}
\end{align}

\noindent Der Ausdruck in \eqref{eq: transformed Korrelation} kann weiter umgeformt werden, indem man den Integranten $e^{A/2}\eta_{i_1} \cdots \eta_{i_k}$ faktorisiert und anschließend die Integration teilweise ausführt. Es sei $\mathcal{N} = \{1,\dots,n\} \times \{1,\dots,n\}$ die Indexmenge der Summation für die Matrix Darstellung \label{eq: Graßmann Transformationsformel} der quadratischen Graßmann Wirkung. Die Mengen $\mathcal{J} = \{n-k+1,\dots,k\} \times \{n+k-1,\dots,k\}$ und $\mathcal{I}^C = \mathcal{N} \setminus \mathcal{I} $ bilden eine Partition von $\mathcal{N}$. Mit dieser Aufteilung der Indexmenge lässt sich der Integrant, unter Ausnutzung der Kommutativität von Paaren von Graßmann Variablen, wie in \eqref{eq: gauschen_integranten_faktoriesieren} faktorisieren.
\begin{align}
    &\exp{ \frac{1}{2} \bm{\theta}^T \bm{\tilde{A}}\, \bm{\theta}\;}  \theta_{1}\cdots\theta_k \nonumber \\
    & = \exp{ \frac{1}{2}\sum_{(i,j) \in \mathcal{N}} \tilde A_{i,j}\, \theta_i, \theta_j\;}  \theta_{1}\cdots\theta_k \nonumber\\
    & = \prod_{(i,j) \in \mathcal{N}} \exp{ \frac{1}{2} \tilde A_{i,j}\, \theta_i, \theta_j\;}  \theta_{1}\cdots\theta_k \nonumber \\
    & = \, \prod_{(i,j) \in \mathcal{J}} \exp{ \frac{1}{2} \tilde A_{i,j}\, \theta_i, \theta_j} \prod_{(i,j)\in \mathcal{J^C}} \left( 1 + \frac{1}{2} \tilde A_{i,j}\, \theta_i, \theta_j\;\right) \theta_{1}\cdots\theta_k \nonumber \\
     & = \left(\, \prod_{(i,j) \in \mathcal{J}} \exp{ \frac{1}{2} \tilde A_{i,j}\, \theta_i, \theta_j}\right) \theta_{1}\cdots\theta_k  \label{eq: gauschen_integranten_faktoriesieren}
\end{align}

\noindent Dabei wurde in \eqref{eq: gauschen_integranten_faktoriesieren} verwendet, dass Quadrate von Graßmann Variablen verschwinden, sodass im Multiplikant nur $1$ einen Beitrag liefert.  
Der Multiplikator hängt nun nicht mehr von $\theta_{1},\dots,\theta_k$ ab, sodass die Integration über $\theta_{1}\cdots\theta_k$ vorgezogen werden kann.
\begin{align}
    &\int d\theta\, \exp{ \frac{1}{2} \bm{\theta}^T \bm{\tilde{A}}\, \bm{\theta}\;}  \theta_{1}\cdots\theta_k \nonumber\\
    &=\int d\theta\, \left(\, \prod_{(i,j) \in \mathcal{J}} \exp{ \frac{1}{2} \tilde A_{i,j}\, \theta_i, \theta_j}\right) \theta_{1}\cdots\theta_k \nonumber \\
    &=  \int d\theta_n \cdots \theta_{n-k+1} \, \left(\prod_{(i,j) \in \mathcal{J}} \exp{ \frac{1}{2} \tilde A_{i,j}\, \theta_i, \theta_j} \int d\theta_k \cdots \theta_{1} \; \theta_{1}\cdots\theta_k \right) \nonumber \\
    &=  \int d\theta_n \cdots \theta_{n-k+1} \, \prod_{(i,j) \in \mathcal{J}} \exp{ \frac{1}{2} \tilde A_{i,j}\, \theta_i, \theta_j} \nonumber \\
    &= pf(\bm{A}^{\cancel{I} \cancel{I}}) \nonumber
\end{align}

\noindent Für die Graßmann Korrelationen ergibt sich somit der Zusammenhang \eqref{eq: calc_GV_Korr_small}. Dieser kann in manchen Fällen, bei günstiger Gestalt der Matrix $\bm{A}$, benutzt werden, um die Korrelationen explizit zu berechnen. Es muss allerdings darauf geachtet werden, dass im Nenner noch das Vorzeichen der Permutation versteckt ist. Diese Formel wird später benutzt um Paar-Korrelationen zu berechnen. 
\begin{equation}
\corr{\eta_{i_1} \cdots \eta_{i_k}}  = \frac{pf(\bm{A}^{\cancel{I} \cancel{I}}) }{pf(\bm{\tilde{A}})} = \frac{pf(\bm{A}^{\cancel{I} \cancel{I}}) }{det(\bm{P}) pf(\bm{A})} \label{eq: calc_GV_Korr_small}
\end{equation}

\noindent Um das Problem analytisch besser zugänglich zu machen, sollen nun Korrelationen von viele Graßmann-Variablen auf Paarkorrelationen zurückgeführt werden. Als erster Schritt wird die Matrix $\bm{\tilde{A}}$ wie in \eqref{eq: Matrixfaktorisierung} faktorisiert.
\begin{equation}
    \bm{\tilde{A}} = 
    \left( \begin{array}{cc}
        \bm{A}^{II}          & \bm{A}^{I \cancel{I}} \\
        \bm{A}^{\cancel{I} I} & \bm{A}^{\cancel{I} \cancel{I}}
    \end{array} \right)
      = 
    \left(\begin{array}{cc} 
        \mathds{1}_{k,k}      &  \bm{G} \\
        0 &  \mathds{1}_{n-k, n-k}
    \end{array}\right)
    \left(\begin{array}{cc} 
        \bm{B}      &  0 \\
        0 &  \bm{A}^{\cancel{I} \cancel{I}}
    \end{array}\right)
    \left(\begin{array}{cc} 
        \mathds{1}_{k,k}        &  0 \\
         \bm{F}  &  \mathds{1}_{n-k, n-k}
    \end{array}\right) \label{eq: Matrixfaktorisierung}
\end{equation}

\noindent Mit den Matrizen:
\begin{align}
\bm{B} &= \bm{A}^{I,I} -\bm{A}^{\cancel{I} I}\, (\bm{A}^{\cancel{I} \cancel{I}})^{-1} \,\bm{A}^{I \cancel{I}} \nonumber\\
\bm{G} &= \bm{A}^{I \cancel{I} }\, (\bm{A}^{\cancel{I} \cancel{I}})^{-1} \nonumber \\
\bm{F} &= (\bm{A}^{\cancel{I} \cancel{I}})^{-1} \,\bm{A}^{\cancel{I} I} \nonumber
\end{align}

\noindent Dabei wird vorausgesetzt, dass der untere Diagnoalblock $\bm{A}^{\cancel{I} \cancel{I}}$ invertierbar ist. Dies kann für nicht verschwindende Korrelationen vorausgesetzt werden. Denn wäre $\bm{A}^{\cancel{I} \cancel{I}}$ nicht invertierbar, so folgt $pf(\bm{A}^{\cancel{I} \cancel{I}}) = \pm\sqrt{det(\bm{A}^{\cancel{I} \cancel{I}})} = 0$ und somit wegen \eqref{eq: calc_GV_Korr_small} dass die Korrelation verschwindet. Es kann auch vorausgesetzt werden, dass $\bm{B}$ invertierbar ist. Denn wäre $\bm{B}$ nicht invertierbar, so folgt aus \eqref{eq: Matrixfaktorisierung} über den Produktsatz für Determinanten, dass $\tilde A$ nicht invertierbar sein kann. Daraus folgt aber, dass $pf(\bm{\tilde{A}}) = 0$ und die Korrelation wäre somit nicht wohldefiniert. 
Für die Inverse Matrix $\bm{\tilde{A}}^{-1}$ ergibt sich dann, mit \eqref{eq: InverseFaktoriesierung}, eine analoge Formel, welche leicht durch nachrechnen verifiziert werden kann.

\begin{equation}
    \tilde{\bm{A}}^{-1} = 
    \left(\begin{array}{cc} 
        \mathds{1}_{k,k}      &  0 \\
        -\bm{F} &  \mathds{1}_{n-k, n-k}
    \end{array}\right)
    \left(\begin{array}{cc} 
        \bm{B}^{-1}      &  0 \\
        0 &  (\bm{A}^{\cancel{I} \cancel{I}})^{-1}
    \end{array}\right)
    \left(\begin{array}{cc} 
        \mathds{1}_{k, k}      &  -\bm{G} \\
         0  &  \mathds{1}_{n-k,n-k}
    \end{array}\right) \label{eq: InverseFaktoriesierung}
\end{equation}

\noindent Aufgrund der Tatsache, dass Permutationen orthogonal sind, dh. $\bm{P}\bm{P}^T=\bm{P}^T\bm{P} = \mathds{1}$, folgt aus $ \mathds{1} = \bm{P}^T\bm{A}^{-1}\bm{P} \bm{P}^T \bm{A}\bm{P}$ der Zusammenhang \eqref{eq: inverse tilde A}, für die inverse der Matrix $\bm{\tilde{A}}$.
\begin{equation}
    \bm{\tilde{A}}^{-1} = \bm{P}^T \bm{A}^{-1} \bm{P} = \left( \begin{array}{cc}
        (\bm{A}^{-1})^{\,II}          & (\bm{A}^{-1})^{\,I \cancel{I}} \\
        (\bm{A}^{-1})^{\,\cancel{I} I} & (\bm{A}^{-1})^{\,\cancel{I} \cancel{I}}
    \end{array} \right) \label{eq: inverse tilde A}
\end{equation}

\noindent Dann erkenn man mit  \eqref{eq: InverseFaktoriesierung} und \eqref{eq: inverse tilde A}, durch einfaches nachrechnen, dass Zusammenhang \eqref{eq: AII is B} gilt.
\begin{equation}
(\bm{A}^{-1})^{\,II} = 
    \left(\begin{array}{cc} 
        \mathds{1}_{K K}      &  0
    \end{array}\right) \bm{\tilde{A}}^{-1}
    \left(\begin{array}{c} 
        \mathds{1}_{K K}      \\
        0
    \end{array}\right) 
    = \bm{B}^{-1} \label{eq: AII is B}
\end{equation}

\noindent Aus der Antisymmetrie der Matrix $\bm{\tilde{A}}$, folgt $ - \bm{A}^{\cancel{I} I} = (\bm{A}^{I\cancel{I}})^T
$ und $ (\bm{A}^{\cancel{I} \cancel{I}})^T = - \bm{A}^{\cancel{I} \cancel{I}}$ und damit $\bm{G}^T = \bm{F}$.
\begin{alignat}{3}
    \bm{G}^T 
    &= (\bm{A}^{I \cancel{I} }\, (\bm{A}^{\cancel{I} \cancel{I}})^{-1})^T 
    &&=((\bm{A}^{\cancel{I} \cancel{I}})^{-1})^T (\bm{A}^{I \cancel{I} })^T &&\nonumber \\
    &= -\,((\bm{A}^{\cancel{I} \cancel{I}})^T))^{-1}\bm{A}^{\cancel{I}I  } 
    &&= (\bm{A}^{\cancel{I} \cancel{I}})^{-1}\bm{A}^{\cancel{I}I  }
    &&= \bm{F} \nonumber
\end{alignat}

\noindent Mithilfe der Transformationsformel \eqref{eq:pfaff 3} und der Diagonalblockformel für Pfaffsche Determinanten \eqref{eq:pfaff 4} folgt:
\begin{equation}
pf(\bm{\tilde{A}}) = det
    \left(\begin{array}{cc} 
        \mathds{1}_{k,k}      &  0 \\
        \bm{F} &  \mathds{1}_{n-k, n-k}
    \end{array}\right)
    det
    \left(\begin{array}{cc} 
        \bm{B}     &  0 \\
        0 &  \bm{A}^{\cancel{I} \cancel{I}}
    \end{array}\right)
    =
    det(\bm{B}) pf( \bm{A}^{\cancel{I} \cancel{I}}) \nonumber
\end{equation}

\noindent Unter Ausnutzung der Formel \eqref{eq:pfaff 5} für die Pfaffsche Determinanten der Inversen Matrix und Gleichung \eqref{eq: AII is B} ergibt sich letztlich:
\begin{equation}
    \corr{\eta_{i_1} \cdots \eta_{i_k}}  = \frac{pf(\bm{A}^{\cancel{I} \cancel{I}}) }{pf(\bm{\tilde{A}})} = pf(\bm{B})^{-1} = pf(-(\bm{B})^{-1}) = pf(-(\bm{A}^{-1})^{\,II}) \nonumber
\end{equation}

\noindent Die Berechnung der Korrelationen lässt sich also wie folgt zusammenfassen:

\begin{grayframe}[frametitle = {Graßmann Korrelationen von Monomen}]
Sei $\bm{A}$ die Antisymmetrische Matrix, welche die Graßmann-Dichte $e^{A/2}$ definiert. Dann gilt:
\begin{equation} \label{eq: Calculate_GV_Corr}
    \corr{\eta_{i_1} \cdots \eta_{i_k}}
    = \frac{pf(\bm{A}^{\cancel{I} \cancel{I}}) }{sign(\sigma) pf(\bm{A})}
    =  \left\{ \begin{array}{cl} 0 & \;\;\text{wenn}\;\; det(\bm{A}^{\cancel{I} \cancel{I}}) = 0 \\ pf(-(\bm{A}^{-1})^{\,II}) & \;\; \text{sonst} \end{array} \right. 
\end{equation} 

\noindent $\sigma: \{1,\dots, n\} \mapsto \{i_i, \dots, i_k, j_1, \dots, j_{n-k}\}$ ist die durch $I = (i_1,\dots,i_k)$ definierte Permutation mit $i_1 < i_2 < \dots < i_{n-k}$ und $j_1 < j_2 < \dots < j_{n-k}$. $\bm{A}^{\cancel{I} \cancel{I}}$ bzw. $\bm{A}^{II}$ sind die Matrizen die man durch Streichen aller Zeilen und Spalten mit Indices in $I$ bzw. nicht in $I$ erhält.
\end{grayframe}

\noindent Betrachtet man die Indizierung $I=(i,j)$ so folgt:
\begin{equation}
\corr{\eta_i\,\eta_j} 
    = pf((-\bm{A}^{-1})^{\,II} )
    = pf \left(                                   -\left(\begin{array}{cc} 
        0 & (A^{-1})_{i,j}     \\
        (A^{-1})_{j,i} & 0
    \end{array}\right) \right)
    = (-1)^2 (A^{-1})_{i,j} 
    = (A^{-1})_{i,j}  \nonumber
\end{equation}

\noindent Dadurch kann die Korrelation von beliebig vielen Variablen auf die Berechnung von Paar-Korrelationen zurückgeführt werden.

\begin{grayframe}[frametitle = {Berechnung von Monom Korrelation mit Paar-Korrelationen}]
Sei $\bm{M}$ die Matrix mit Einträgen $M_{j,l} = \corr{\eta_{i_j} \eta_{i_l}}$ so gilt:

\begin{equation} \label{eq: calc monom corr with pair corr matrix}
    \corr{\eta_{i_1} \cdots \eta_{i_k}}
    =  pf(\bm{M})
\end{equation} 

\end{grayframe}